{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Table of contents\n",
    "\n",
    "#### 1. [Package instalation (optional)](#1)\n",
    "#### 2. [Data loading](#2)\n",
    "#### 3. [EDA](#3)\n",
    "- ##### 3.1. [Understanding data types](#3_1)\n",
    "- ##### 3.2. [Understanding features](#3_2)\n",
    "- - ##### 3.2.1. [Content score](#3_2_1)\n",
    "- - ##### 3.2.2. [Wording score](#3_2_2)\n",
    "- - ##### 3.2.3. [Content vs Wording](#3_2_3)\n",
    "- ##### 3.3. [Tokenization](#3_3)\n",
    "- ##### 3.4. [Word and character count](#3_4)\n",
    "- - ##### 3.4.1. [prompts_train dataset](#3_4_1)\n",
    "- - ##### 3.4.2. [summaries_train dataset](#3_4_2)\n",
    "- ##### 3.5. [Word and character count after removing stopwords](#3_5)\n",
    "- - ##### 3.5.1. [prompts_train dataset](#3_5_1)\n",
    "- - ##### 3.5.2. [summaries_train dataset](#3_5_2)\n",
    "- ##### 3.6. [N-gram analysis](#3_6)\n",
    "- - ##### 3.6.1. [prompts_train dataset](#3_6_1)\n",
    "- - ##### 3.6.2. [summaries_train dataset](#3_6_2)\n",
    "- ##### 3.7. [Word Clouds](#3_7)\n",
    "- - ##### 3.7.1. [prompts_train dataset](#3_7_1)\n",
    "- - ##### 3.7.2. [summaries_train dataset](#3_7_2)\n",
    "- ##### 3.8. [Sentiment Analysis](#3_8)\n",
    "- - ##### 3.8.1. [prompts_train dataset](#3_8_1)\n",
    "- - - ##### 3.8.1.1 [Polarity](#3_8_1_1)\n",
    "- - - ##### 3.8.1.2 [Subjectivity](#3_8_1_2)\n",
    "- - - ##### 3.8.1.3 [Readability](#3_8_1_3)\n",
    "- - ##### 3.8.2. [summaries_train dataset](#3_8_2)\n",
    "- - - ##### 3.8.2.1 [Polarity](#3_8_2_1)\n",
    "- - - ##### 3.8.2.2 [Subjectivity](#3_8_2_2)\n",
    "- - - ##### 3.8.2.3 [Readability](#3_8_2_3)\n",
    "- ##### 3.9. [Spelling Analysis](#3_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## 1. Package installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have these packages installed you can simply leave all the lines commented out. Otherwise, uncomment everything out and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install plotly\n",
    "#!pip install nbformat --upgrade\n",
    "#!pip install plotly cufflinks\n",
    "#!pip install wordcloud\n",
    "#!pip install textblob\n",
    "#!pip install textstat\n",
    "#!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only concentrate on train data: \"summaries_train.csv\" and \"prompts_train.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df = pd.read_csv('../data/summaries_train.csv')\n",
    "prompts_train_df = pd.read_csv('../data/prompts_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the content of both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"summaries_train dataframe:\")\n",
    "display(summaries_train_df.head(1))\n",
    "print(\"prompts_train dataframe:\")\n",
    "display(prompts_train_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_1\"></a>\n",
    "#### 3.1 Understanding data types\n",
    "\n",
    "For better understanding of the data, it is worth to understand datatypes used in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_type(obj):\n",
    "    if isinstance(obj, (list, tuple, set, pd.core.series.Series)):\n",
    "        if obj:\n",
    "            return type(obj).__name__ + \" of \" + deep_type(obj[0])\n",
    "        else:\n",
    "            return type(obj).__name__\n",
    "    else:\n",
    "        return type(obj).__name__\n",
    "\n",
    "def print_column_types(df):\n",
    "    # Determine the maximum column name length for alignment\n",
    "    max_col_len = max(len(col) for col in df.columns)\n",
    "\n",
    "    for column in df.columns:\n",
    "        first_non_na = df[column].dropna().iloc[0] if not df[column].isna().all() else None\n",
    "        if first_non_na is not None:\n",
    "            print(f\"column: {column:<{max_col_len}} \\t type: {deep_type(first_non_na)}\")\n",
    "        else:\n",
    "            print(f\"column: {column:<{max_col_len}} \\t type: {type(first_non_na).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"summaries_train dataframe:\\n\")\n",
    "print_column_types(summaries_train_df)\n",
    "display(summaries_train_df.head(1))\n",
    "\n",
    "print(\"prompts_train dataframe:\\n\")\n",
    "print_column_types(prompts_train_df)\n",
    "display(prompts_train_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_2\"></a>\n",
    "#### 3.2 Understanding features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fully understand data, one must understand each feature of the dataset.  \n",
    "This is discussed next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of entries in the summaries_train_df: {len(summaries_train_df)}')\n",
    "print(f\"Number of unique 'student_id' values in summaries_train_df: {len(summaries_train_df['student_id'].unique())}\")\n",
    "print(f\"Number of unique 'prompt_id' values in summaries_train_df: {len(summaries_train_df['prompt_id'].unique())}\")\n",
    "print(f\"Number of summaries in summaries_train_df: {len(summaries_train_df['text'])}\\n\")\n",
    "\n",
    "print(f'Number of entries in the prompts_train_df: {len(prompts_train_df)}')\n",
    "print(f\"Number of unique 'prompt_id' values in prompts_train_df: {len(prompts_train_df['prompt_id'].unique())}\")\n",
    "print(f\"Number of 'prompt_question' values in prompts_train_df: {len(prompts_train_df['prompt_question'].unique())}\")\n",
    "print(f\"Number of 'prompt_title' values in prompts_train_df: {len(prompts_train_df['prompt_title'].unique())}\\n\")\n",
    "\n",
    "print(f\"Unique 'prompt_id' values in summaries_train_df: {summaries_train_df['prompt_id'].unique()}\")\n",
    "print(f\"Unique 'prompt_id' values in prompts_train_df: {prompts_train_df['prompt_id'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are in total 7165 entries in \"summaries_train_df\", each written by a different student.  \n",
    "Every summary (\"text\" column in \"summaries_train_df\") is written according to 1 of 4 questions/tasks defined in the \"prompt_question\" column of \"prompts_train_df\". Every \"prompt_question\" corresponds to exactly one \"prompt_title\" (also in \"prompts_train_df\") which describes it shortly.  \n",
    "Finally, \"prompt_text\" column (also in \"prompts_train_df\") stores the full texts that students have to summarize. \n",
    "  \n",
    "For every summary, student is awarded content score (\"content\" column in summaries_train_df) and wording score (\"wording\" column in \"summaries_train_df\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_2_1\"></a>\n",
    "##### 3.2.1 Content score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on competition host's notes, the content score is accounting for 3 factors:\n",
    "\n",
    "- Main idea (i.e. How well did the summary capture the main idea of the source?)  \n",
    "\n",
    "- Details (i.e. How accurately did the summary capture the details from the source?)  \n",
    "\n",
    "- Cohesion (i.e. How well did the summary transition from one idea to the next?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['content'].iplot(\n",
    "    kind='hist',\n",
    "    bins=60,\n",
    "    layout=dict(\n",
    "        title='Content score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='content score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content score ranges from roughly -2 to +4 which points to possible data transformation. This can be checked by looking at the column summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['content'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By closer inspection, one can see that the range of the content score is between -1.73 and 3.9. The mean and standard deviation are very close to 0 and 1, respectively, which points to the data standardization.  \n",
    "\n",
    "At this point, it is not clear how these scores map to actual grades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_2_2\"></a>\n",
    "##### 3.2.2 Wording score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on competition host's notes, the wording score is accounting for 3 factors:\n",
    "\n",
    "- Voice (i.e. Was the summary written using objective language?)  \n",
    "\n",
    "- Paraphrase (i.e. Is the summary properly paraphrased?)  \n",
    "\n",
    "- Language (i.e. How well did the summary use lexis and the syntax) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['wording'].iplot(\n",
    "    kind='hist',\n",
    "    bins=60,\n",
    "    layout=dict(\n",
    "        title='Wording score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='wording score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wording score ranges roughly from -2 to 4.3. It is reasonable to expect that the same data transformation technique was applied on the wording score as on the content score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['wording'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum value for wording is -1.96, while the maximum value is 4.31. As expected, mean and standard deviation are very close to 0 and 1, respectively, which suggests that data standardization was applied on wording score as well.\n",
    "\n",
    "As is the case for content score, it is not clear, at this moment, the meaning behind content score values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_2_3\"></a>\n",
    "##### 3.2.3 Content vs Wording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at content score distribution, we can see that the wording score distribution is multimodal, although this is harder to see due to corse binning. The same can be said for wording score distribution.  \n",
    "\n",
    "One can see clusters better if the number of binning is increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['content'].iplot(\n",
    "    kind='hist',\n",
    "    bins=360,\n",
    "    layout=dict(\n",
    "        title='Content score distribution (finer binning)',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='content score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "summaries_train_df['wording'].iplot(\n",
    "    kind='hist',\n",
    "    bins=360,\n",
    "    layout=dict(\n",
    "        title='Wording score distribution (finer binning)',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='wording score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, there appears to be several overlapping Gauss-like distributions which points to possible existence of clusters. One thing to try is to prepare a (content, wording) scatter plot to find possible clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "scatter_plot = go.Scatter(\n",
    "    x=summaries_train_df['content'],\n",
    "    y=summaries_train_df['wording'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='blue'),  # Adjust the size as needed\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title='content score'),\n",
    "    yaxis=dict(title='Wording Score'),\n",
    "    title='Content score vs. Wording score',\n",
    "    title_x=0.5,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[scatter_plot], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plot, it can be seen that there exists 37 clusters. This is more clear if on applies 30° rotation on both content and wording scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "angle_deg = 30\n",
    "angle_rad = np.radians(angle_deg)\n",
    "rotation_matrix = np.array([[np.cos(angle_rad), -np.sin(angle_rad)],\n",
    "                            [np.sin(angle_rad), np.cos(angle_rad)]])\n",
    "\n",
    "rotated_values = np.dot(summaries_train_df[['content', 'wording']].values, rotation_matrix)\n",
    "summaries_train_df['content_rotated'] = rotated_values[:, 0]\n",
    "summaries_train_df['wording_rotated'] = rotated_values[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotated content score\n",
    "scatter_plot = go.Scatter(\n",
    "    x=summaries_train_df['content_rotated'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='blue'),  # Adjust the size as needed\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title='content score'),\n",
    "    #yaxis=dict(title='Wording Score'),\n",
    "    title='Rotated content score',\n",
    "    title_x=0.5,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[scatter_plot], layout=layout)\n",
    "fig.show()\n",
    "\n",
    "# rotated wording score\n",
    "scatter_plot = go.Scatter(\n",
    "    x=summaries_train_df['wording_rotated'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='blue'),  # Adjust the size as needed\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title='wording score'),\n",
    "    #yaxis=dict(title='Wording Score'),\n",
    "    title='Rotated wording score',\n",
    "    title_x=0.5,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[scatter_plot], layout=layout)\n",
    "fig.show()\n",
    "\n",
    "# rotated content score vs. rotated wording score\n",
    "scatter_plot = go.Scatter(\n",
    "    x=summaries_train_df['content_rotated'],\n",
    "    y=summaries_train_df['wording_rotated'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color='blue'),  # Adjust the size as needed\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title='content score'),\n",
    "    yaxis=dict(title='Wording Score'),\n",
    "    title='Content score vs. Wording score',\n",
    "    title_x=0.5,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[scatter_plot], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['content_rotated'].iplot(\n",
    "    kind='hist',\n",
    "    bins=360,\n",
    "    layout=dict(\n",
    "        title='Rotated content score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='content score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "summaries_train_df['wording'].iplot(\n",
    "    kind='hist',\n",
    "    bins=360,\n",
    "    layout=dict(\n",
    "        title='Wording score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='wording score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "summaries_train_df['wording_rotated'].iplot(\n",
    "    kind='hist',\n",
    "    bins=360,\n",
    "    layout=dict(\n",
    "        title='Rotated wording score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='wording score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "summaries_train_df['content'].iplot(\n",
    "    kind='hist',\n",
    "    bins=360,\n",
    "    layout=dict(\n",
    "        title='Content score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='content score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data is rotated by 30° it becomes clear that the content score is divided into 37 distinct clusters. The question remains as to what is the meaning of these clusters. One possible explanation could be the following.  \n",
    "\n",
    "The basic grading system in the US is: A, B, C, D, F. However, if we go to a finer-grained system we have A+, A, A−, B+, B, B−, C+, C, C−, D+, D, D− and F. Looking at the rotated content score distribution, one can see that there exists one particularly bad score with 426 entries. Let's ignore that group. This leaves us with 36 bins. Ignoring grade F (the only one without finer grading), there are 12 possible grades. This could suggest that each group of 3 clusters (starting from the second on the left) corresponds to one finer-grained score (A+, A, A−, etc.). The remaining bin (the first bin to the right) could correspond to grade F. If this is correct, one could simply map content score to percentage (written as decimal number in range 0-1) corresponding to each of the grades.  \n",
    "\n",
    "Additional thing that one might notice is that, by doing rotation by 90° (instead of 30°), rotated content score distribution becomes (non-rotated) wording score distribution. Similarly, by doing rotation of -90°, rotated wording score distribution becomes (non-rotated) content score distribution. While this is clear from geometric point of view (i.e. we are just swapping x and y axis), it is not clear if there is a deeper meaning to this. Why is content score the exact same thing as wording score rotated by 90°. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_3\"></a>\n",
    "#### 3.3 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a process of converting text into a list of words. This is the crucial step for exploring the number of words in a given text, number of characters per word, performing word frequency analysis, counting and exploring stopping words, looking into N-grams, performing topic modelling, doing sentiment analysis, calculating text's readability score, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the 'punkt' resource\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_question_tokenized'] = prompts_train_df['prompt_question'].apply(word_tokenize)\n",
    "prompts_train_df['prompt_title_tokenized'] = prompts_train_df['prompt_title'].apply(word_tokenize)\n",
    "prompts_train_df['prompt_text_tokenized'] = prompts_train_df['prompt_text'].apply(word_tokenize)\n",
    "\n",
    "summaries_train_df['text_tokenized'] = summaries_train_df['text'].apply(word_tokenize)\n",
    "\n",
    "print(\"summaries_train dataframe:\\n\")\n",
    "display(summaries_train_df.sample(1))\n",
    "\n",
    "print(\"prompts_train dataframe:\\n\")\n",
    "display(prompts_train_df.sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_4\"></a>\n",
    "#### 3.4 Word and character count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_4_1\"></a>\n",
    "##### 3.4.1 prompts_train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_question'].str.len().iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Number of characters per prompt question',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='character count', range=[70, 190]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_question_tokenized'].apply(len).iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Number of words per prompt question',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='word count', range=[10, 35]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "(prompts_train_df['prompt_question'].str.len()/prompts_train_df['prompt_question_tokenized'].apply(len)).iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Number of characters per word in prompt question',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='characters per word', range=[5, 6]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 4 entries in prompts_train_df which is reflected on the histograms. Prompt questions have between 77 and 184 characters and between 15 and 31 words. It must be noted that characters like \",\", \".\", \"?\", etc. are counted as words. Words are roughly 5 characters long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_title'].str.len().iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Number of characters per prompt title',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='character count', range = [9, 26]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_title_tokenized'].apply(len).iplot(\n",
    "    kind='hist',\n",
    "    bins=60,\n",
    "    layout=dict(\n",
    "        title='Number of words per prompt title',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='word count', range = [1, 5]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "(prompts_train_df['prompt_title'].str.len()/prompts_train_df['prompt_title_tokenized'].apply(len)).iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Number of characters per word in prompt title',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='characters per word', range = [4, 9]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the number of words and, subsequently, characters per prompt title is smaller compared to prompt question. However, slightly longer words are used in the prompt title compared to prompt question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_text'].str.len().iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Number of characters per prompt text',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='character count', range = [3300, 5140]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_text_tokenized'].apply(len).iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Number of words per prompt text',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='word count', range = [620, 1080]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "(prompts_train_df['prompt_text'].str.len()/prompts_train_df['prompt_text_tokenized'].apply(len)).iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Number of characters per word in prompt text',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='characters per word', range = [4.5, 5.5]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three texts are around 3500 characters and 650 words long, while one is around 5000 characters and 1000 words long. Still, the average word length is around 5 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_4_2\"></a>\n",
    "##### 3.4.2 summaries_train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['text'].str.len().iplot(\n",
    "    kind='hist',\n",
    "    bins=90,\n",
    "    layout=dict(\n",
    "        title='Number of characters per summary text',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='character count'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "summaries_train_df['text_tokenized'].apply(len).iplot(\n",
    "    kind='hist',\n",
    "    bins=90,\n",
    "    layout=dict(\n",
    "        title='Number of words per summary text',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='word count'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "(summaries_train_df['text'].str.len()/summaries_train_df['text_tokenized'].apply(len)).iplot(\n",
    "    kind='hist',\n",
    "    bins=90,\n",
    "    layout=dict(\n",
    "        title='Number of characters per word in summary text',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='characters per word'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the number of words and number of characters in the student summaries follow a log-normal distribution with peaks at around 170 characters and 35 words per summary. Number of characters per word is normally distributed with mean around 5 characters per word and exhibits a slightly positive skew (i.e. right tail).  \n",
    "\n",
    "Looking globally, summaries are ~5% of the length of full text (170 compared to 3500 characters or 35 compared to 600 words). However, the average word length in both prompt texts and student summaries is ~5 characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_5\"></a>\n",
    "#### 3.5 Word and character count after removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are words that are commonly used in a language. In English, these are \"the\", \"and, \"a\", \"of\"... It is useful to remove them from corpus before moving on with text analysis since they (usually) don't add any valuable information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "def filter_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop and word.isalnum()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_5_1\"></a>\n",
    "##### 3.5.1 prompts_train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with only stopwords\n",
    "prompts_train_df['prompt_question_stopwords'] = prompts_train_df['prompt_question_tokenized'].apply(lambda x: [word for word in x if word in stop])\n",
    "# Create a column with only non-stopwords\n",
    "prompts_train_df['prompt_question_non_stopwords'] = prompts_train_df['prompt_question_tokenized'].apply(filter_stopwords)\n",
    "\n",
    "# Create a column with only stopwords\n",
    "prompts_train_df['prompt_title_stopwords'] = prompts_train_df['prompt_title_tokenized'].apply(lambda x: [word for word in x if word in stop])\n",
    "# Create a column with only non-stopwords\n",
    "prompts_train_df['prompt_title_non_stopwords'] = prompts_train_df['prompt_title_tokenized'].apply(filter_stopwords)\n",
    "\n",
    "# Create a column with only stopwords\n",
    "prompts_train_df['prompt_text_stopwords'] = prompts_train_df['prompt_text_tokenized'].apply(lambda x: [word for word in x if word in stop])\n",
    "# Create a column with only non-stopwords\n",
    "prompts_train_df['prompt_text_non_stopwords'] = prompts_train_df['prompt_text_tokenized'].apply(filter_stopwords)\n",
    "\n",
    "display(prompts_train_df.sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_question_stopwords'].str.len().iplot(\n",
    "    kind='hist',\n",
    "    bins=60,\n",
    "    layout=dict(\n",
    "        title='Number of stopwords per prompt question',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='stopword count'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_title_stopwords'].str.len().iplot(\n",
    "    kind='hist',\n",
    "    bins=60,\n",
    "    layout=dict(\n",
    "        title='Number of stopwords per prompt title',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='stopword count'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_text_stopwords'].str.len().iplot(\n",
    "    kind='hist',\n",
    "    bins=60,\n",
    "    layout=dict(\n",
    "        title='Number of stopwords per prompt text',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='stopword count', range = [200, 500]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cunting how often each stopword occurs\n",
    "\n",
    "#flatten the list of lists of stopwords\n",
    "unique_prompt_question_stopwords = [word for sublist in prompts_train_df['prompt_question_stopwords'] for word in sublist]\n",
    "# convert to a pandas Series and get frequencies\n",
    "unique_prompt_question_stopwords_series = pd.Series(unique_prompt_question_stopwords).value_counts()\n",
    "\n",
    "#flatten the list of lists of stopwords\n",
    "unique_prompt_title_stopwords = [word for sublist in prompts_train_df['prompt_title_stopwords'] for word in sublist]\n",
    "# convert to a pandas Series and get frequencies\n",
    "unique_prompt_title_stopwords_series = pd.Series(unique_prompt_title_stopwords).value_counts()\n",
    "\n",
    "#flatten the list of lists of stopwords\n",
    "unique_prompt_text_stopwords = [word for sublist in prompts_train_df['prompt_text_stopwords'] for word in sublist]\n",
    "# convert to a pandas Series and get frequencies\n",
    "unique_prompt_text_stopwords_series = pd.Series(unique_prompt_text_stopwords).value_counts()\n",
    "\n",
    "unique_prompt_question_stopwords_series.iplot(\n",
    "    kind='bar',\n",
    "    layout=dict(\n",
    "        title='Frequency of stopwords in prompt question',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='Stopword'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "unique_prompt_title_stopwords_series.iplot(\n",
    "    kind='bar',\n",
    "    layout=dict(\n",
    "        title='Frequency of stopwords in prompt title',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='Stopword'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "unique_prompt_text_stopwords_series.iplot(\n",
    "    kind='bar',\n",
    "    layout=dict(\n",
    "        title='Frequency of stopwords in prompt text',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='Stopword'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_question_non_stopwords'].str.len().iplot(\n",
    "    kind='hist',\n",
    "    bins=60,\n",
    "    layout=dict(\n",
    "        title='Number of words per prompt question (without stopwords)',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='stopword count'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_title_non_stopwords'].str.len().iplot(\n",
    "    kind='hist',\n",
    "    bins=60,\n",
    "    layout=dict(\n",
    "        title='Number of words per prompt title (without stopwords)',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='stopword count'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_text_non_stopwords'].str.len().iplot(\n",
    "    kind='hist',\n",
    "    bins=60,\n",
    "    layout=dict(\n",
    "        title='Number of words per prompt text (without stopwords)',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='stopword count', range = [200, 500]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(prompts_train_df['prompt_question'].str.len()/prompts_train_df['prompt_question_non_stopwords'].apply(len)).iplot(\n",
    "    kind='hist',\n",
    "    bins=60,\n",
    "    layout=dict(\n",
    "        title='Number of characters per word in prompt question (without stopwords)',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='character count', range = [8, 12]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "(prompts_train_df['prompt_title'].str.len()/prompts_train_df['prompt_title_non_stopwords'].apply(len)).iplot(\n",
    "    kind='hist',\n",
    "    bins=60,\n",
    "    layout=dict(\n",
    "        title='Number of characters per word in prompt title (without stopwords)',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='character count', range = [4, 9]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "(prompts_train_df['prompt_text'].str.len()/prompts_train_df['prompt_text_non_stopwords'].apply(len)).iplot(\n",
    "    kind='hist',\n",
    "    bins=60,\n",
    "    layout=dict(\n",
    "        title='Number of characters per word in prompt text (without stopwords)',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='character count', range = [9.9, 11.5]),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing stopwords, there are 8-18 words in prompt question, compared to 15-31 words prior to removing stopwords. The three most used stopwords in prompt question are \"the\", \"of\" and \"in\". The word length increased after removing stopwords (from roughly 5 characters to around 10). This is expected since stopwords are short.\n",
    "\n",
    "In three out of four prompt titles, 3 words were used after removing stopwords, similar to what we had before removing stopwords. This is expected since prompt titles are very short. Consequently, the word length is also very similar. Only one stopword was removed: \"from\".  \n",
    "\n",
    "The biggest difference is seen in prompt text where, prior to removing stopwords, there were around 650 words per prompt text. This reduced to roughly 400 words after removing stopwords. This reflects in the word length as well. Before removing stopwords, the average word length was 5 characters, while it is around 11 characters after removing stopwords. This is expected since the most often used stopwords, three of which are \"the\", \"and\" and \"a\", are very short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_5_2\"></a>\n",
    "##### 3.5.2 summaries_train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with only stopwords\n",
    "summaries_train_df['text_stopwords'] = summaries_train_df['text_tokenized'].apply(lambda x: [word for word in x if word in stop])\n",
    "# Create a column with only non-stopwords\n",
    "summaries_train_df['text_non_stopwords'] = summaries_train_df['text_tokenized'].apply(filter_stopwords)\n",
    "\n",
    "display(summaries_train_df.sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['text_stopwords'].str.len().iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Number of stopwords per summary text',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='stopword count'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "#flatten the list of lists of stopwords\n",
    "unique_summary_stopwords = [word for sublist in summaries_train_df['text_stopwords'] for word in sublist]\n",
    "# convert to a pandas Series and get frequencies\n",
    "unique_summary_stopwords_series = pd.Series(unique_summary_stopwords).value_counts()\n",
    "\n",
    "unique_summary_stopwords_series.iplot(\n",
    "    kind='bar',\n",
    "    layout=dict(\n",
    "        title='Frequency of stopwords in summary text',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='Stopword'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of stopwords per sumary follows a log-normal distribution centered at around 17 stopwords per summary and with long right tail. The three most common stopwords are \"the\", \"to\" and \"a\". This is very similar to what we have in prompt question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['text_non_stopwords'].str.len().iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Number of words per summary (without stopwords)',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='stopword count'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "(summaries_train_df['text'].str.len()/summaries_train_df['text_non_stopwords'].apply(len)).iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Number of characters per word in summary (without stopwords)',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='character count'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of words per summary follows a log-normal distribution peaking at around 17 words per summary and exhibits a long right tail. Word length is normally distributed with the average word being 10.5 characters long.  \n",
    "This can be compared with an average of 35 words per summary and word length of 5 characters before removing stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_6\"></a>\n",
    "#### 3.6 N-gram analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An n-gram is a collection of n successive items in a text document that may include words, numbers, symbols, and punctuation. N-gram models are useful in many text analytics applications where sequences of words are relevant, such as in sentiment analysis, text classification, and text generation. N-gram modeling is one of the many techniques used to convert text from an unstructured format to a structured format.  \n",
    "\n",
    "For example, let's look at the sentence \"Eating too much sugar is not good for your health.\" The list of words is [\"Eating\", \"too\", \"much\", \"sugar\", \"is\", \"not\", \"good\", \"for\", your\", \"health\"].  \n",
    "\n",
    "The 1-grams are eating, too, much, etc... By looking only at individual words, meaning can be lost. For example, by looking at 2-grams: eating too, too much, much sugar, sugar is, is not, not good, good for,... the understanding can be different (e.g. not good vs good). Ading 3-grams, 4-grams, etc. can sometimes be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_6_1\"></a>\n",
    "##### 3.6.1 prompts_train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "def plot_ngram_histogram(corpus, ngram_length=1, top_n=20, color='blue', column = None):\n",
    "    \n",
    "    # Generate N-grams\n",
    "    ngrams_list = list(ngrams(corpus, ngram_length))\n",
    "    \n",
    "    # Count the frequency of each N-gram\n",
    "    ngrams_counts = Counter(ngrams_list)\n",
    "    \n",
    "    # Sort the N-grams by count in descending order\n",
    "    sorted_ngrams = dict(sorted(ngrams_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # Extract the top N-grams and their counts\n",
    "    top_ngrams = list(sorted_ngrams.items())[:top_n]\n",
    "\n",
    "    # Create a Pandas Series from the top bigrams\n",
    "    ngram_series = pd.Series({ngram: count for ngram, count in top_ngrams})\n",
    "\n",
    "    # Plot the most frequent bigrams using iplot\n",
    "    ngram_series.iplot(\n",
    "    kind='bar',\n",
    "    layout = dict(\n",
    "        title = f'{ngram_length}-grams in {column}',\n",
    "        title_x = 0.5,\n",
    "        xaxis = dict(title = f'{ngram_length}-grams'),\n",
    "        yaxis = dict(title = f'Frequency of {ngram_length}-grams')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyse words in prompt text only. Prompt title and prompt question are less relevant since they are very short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the list of lists of words to create corpus\n",
    "corpus_prompt_text = [word for sublist in prompts_train_df['prompt_text_non_stopwords'] for word in sublist]\n",
    "# convert to a pandas Series and get frequencies\n",
    "words_series_prompt_text = pd.Series(corpus_prompt_text).value_counts()\n",
    "\n",
    "#plotting the most frequent words\n",
    "words_series_prompt_text[:30].iplot(\n",
    "    kind='bar',\n",
    "    layout = dict(\n",
    "        title = 'Word frequency in prompt text',\n",
    "        title_x = 0.5,\n",
    "        xaxis = dict(title = 'word'),\n",
    "        yaxis = dict(title = 'count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngram_histogram(corpus_prompt_text, ngram_length=2, top_n=20, color='blue', column='prompt text')\n",
    "plot_ngram_histogram(corpus_prompt_text, ngram_length=3, top_n=20, color='blue', column = 'prompt text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_6_2\"></a>\n",
    "##### 3.6.2 summaries_train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we analyze student summaries to extract bi-grams and tri-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the list of lists of words to create corpus\n",
    "corpus_sumaries = [word for sublist in summaries_train_df['text_non_stopwords'] for word in sublist]\n",
    "# convert to a pandas Series and get frequencies\n",
    "words_series_summaries = pd.Series(corpus_sumaries).value_counts()\n",
    "\n",
    "#plotting the most frequent words\n",
    "words_series_summaries[:30].iplot(\n",
    "    kind='bar',\n",
    "    layout = dict(\n",
    "        title = 'Word frequency in student summaries',\n",
    "        title_x = 0.5,\n",
    "        xaxis = dict(title = 'word'),\n",
    "        yaxis = dict(title = 'count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ngram_histogram(corpus_sumaries, ngram_length=2, top_n=20, color='blue', column='student summaries')\n",
    "plot_ngram_histogram(corpus_sumaries, ngram_length=3, top_n=20, color='blue', column = 'student summaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most often used words in prompt texts are \"Would\", \"Jones\", \"The\", \"students\", \"meat\", \"could\" and \"movement\".  \n",
    "The most often used words in student summaries are \"would\", \"The\", \"meat\", \"tragedy\", \"good\", \"spoiled\" and \"bad\".  \n",
    "\n",
    "The most frequent bi-grams in prompt texts are (pitty, fear), (Third, Wawe), (man, could), (change, fortune) and (neither, pitty).  \n",
    "The most frequent bi-grams in student summaries are (spoiled, meat), (ideal, tragedy), (pitty, fear), (good, bad) and (They, would).\n",
    "\n",
    "The most frequent tri-grams in prompt texts are (neither, pitty, fear), (good, bad, It), (The, Third, Wawe), (There, would, meat) and (Chapter, 13, As).  \n",
    "The most frequent tri-grams in student summaries are (soda, take away), (take, away, smell), (would, rub, soda), (rub, soda, take) and (cover, spoiled, meat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_7\"></a>\n",
    "#### 3.7 Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Clouds are often a good, graphical, method of getting a fast insight into most often used words in a corpus of text and can give the idea of the context, and overall sentiment of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords_wordcloud = set(STOPWORDS)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_wordcloud(data):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords_wordcloud,\n",
    "        max_words=30, #100\n",
    "        max_font_size=100, #30\n",
    "        scale=3,\n",
    "        random_state=1)\n",
    "\n",
    "    wordcloud=wordcloud.generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_7_1\"></a>\n",
    "##### 3.7.1 prompts_train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to use infinitives instead of inflected forms of word in text analysis. This is done through \"Lemmatization\". Lemmatization is the process of grouping together different inflected forms of the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infinitives_prompt_text = [lem.lemmatize(w) for w in corpus_prompt_text if len(w)>3] # lemmatizing only words longer than 3 characters\n",
    "show_wordcloud(infinitives_prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_7_2\"></a>\n",
    "##### 3.7.2 summaries_train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infinitives_summaries = [lem.lemmatize(w) for w in corpus_sumaries if len(w)>3] # lemmatizing only words longer than 3 characters\n",
    "show_wordcloud(infinitives_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_8\"></a>\n",
    "#### 3.8 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_8_1\"></a>\n",
    "##### 3.8.1 prompts_train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For analysing text sentiment we will use textblob library: a Python library built on top of NLTK. textblob calculates two different scores for a given text:  \n",
    "\n",
    "- polarity: a floating-point number that lies in the range of [-1,1], where -1 means a negative statement and 1 means positive statement.  \n",
    "\n",
    "- subjectivity: a floating-point number that lies in the range of [0,1] used to describe how someone’s judgment is shaped by personal opinions and feelings. Score 0 is attributed to the text that is fact-oriented, while score 1 points to the highly opinion-driven text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_8_1_1\"></a>\n",
    "- ##### 3.8.1.1 Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def sentiment(x):\n",
    "    if x < -0.1:\n",
    "        return 'neg'\n",
    "    elif x >= -0.1 and x <= 0.1:\n",
    "        return 'neu'\n",
    "    else:\n",
    "        return 'pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_question_polarity_score'] = prompts_train_df['prompt_question'].apply(lambda x: polarity(x))\n",
    "prompts_train_df['prompt_title_polarity_score'] = prompts_train_df['prompt_title'].apply(lambda x: polarity(x))\n",
    "prompts_train_df['prompt_text_polarity_score'] = prompts_train_df['prompt_text'].apply(lambda x: polarity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_question_polarity_score'].iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Prompt question polarity score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='polarity score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_title_polarity_score'].iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Prompt title polarity score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='polarity score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_text_polarity_score'].iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Prompt text polarity score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='polarity score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveness_prompt_question = prompts_train_df['prompt_question_polarity_score'].apply(lambda x: sentiment(x))\n",
    "positiveness_prompt_title = prompts_train_df['prompt_title_polarity_score'].apply(lambda x: sentiment(x))\n",
    "positiveness_prompt_text = prompts_train_df['prompt_text_polarity_score'].apply(lambda x: sentiment(x))\n",
    "\n",
    "positiveness_prompt_question.iplot(\n",
    "    kind='hist',\n",
    "    bins=3,\n",
    "    layout=dict(\n",
    "        title='Prompt question sentiment',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='sentiment'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "positiveness_prompt_title.iplot(\n",
    "    kind='hist',\n",
    "    bins=3,\n",
    "    layout=dict(\n",
    "        title='Prompt title sentiment',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='sentiment'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "positiveness_prompt_text.iplot(\n",
    "    kind='hist',\n",
    "    bins=3,\n",
    "    layout=dict(\n",
    "        title='Prompt text sentiment',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='sentiment'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_8_1_2\"></a>\n",
    "- ##### 3.8.1.2 Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_question_subjectivity_score'] = prompts_train_df['prompt_question'].apply(lambda x: subjectivity(x))\n",
    "prompts_train_df['prompt_title_subjectivity_score'] = prompts_train_df['prompt_title'].apply(lambda x: subjectivity(x))\n",
    "prompts_train_df['prompt_text_subjectivity_score'] = prompts_train_df['prompt_text'].apply(lambda x: subjectivity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_question_subjectivity_score'].iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Prompt question subjectivity score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='subjectivity score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_title_subjectivity_score'].iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Prompt title subjectivity score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='subjectivity score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_text_subjectivity_score'].iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Prompt text subjectivity score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='subjectivity score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, prompt questions, titles and texts are neutral and (mostly) non-opinionated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_8_1_3\"></a>\n",
    "- ##### 3.8.1.3 Readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A readability score is a metric that indicates how difficult (or easy) it is to read and understand a text. The following legend can help understanding the results obtained.  \n",
    "\n",
    "90-100\tvery easy to read, easily understood by an average 11-year-olds\n",
    "\n",
    "80-90\teasy to read\n",
    "\n",
    "70-80\tfairly easy to read\n",
    "\n",
    "60-70\teasily understood by 13- to 15-year-olds\n",
    "\n",
    "50-60\tfairly difficult to read\n",
    "\n",
    "30-50\tdifficult to read, best understood by college graduates\n",
    "\n",
    "0-30\tvery difficult to read, best understood by university graduates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textstat import flesch_reading_ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_question_readability_score'] = prompts_train_df['prompt_question'].apply(lambda x: flesch_reading_ease(x))\n",
    "prompts_train_df['prompt_title_readability_score'] = prompts_train_df['prompt_title'].apply(lambda x: flesch_reading_ease(x))\n",
    "prompts_train_df['prompt_text_readability_score'] = prompts_train_df['prompt_text'].apply(lambda x: flesch_reading_ease(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_question_readability_score'].iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Prompt question subjectivity score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='readability score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_title_readability_score'].iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Prompt title subjectivity score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='readability score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')\n",
    "\n",
    "prompts_train_df['prompt_text_readability_score'].iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Prompt text subjectivity score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='readability score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis shows that questions are formatted so that they are not too difficult to understand. Titles and texts are (mostly) very easy to understand with only one entry slightly more challenging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_8_2\"></a>\n",
    "##### 3.8.2 prompts_train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_8_2_1\"></a>\n",
    "- ##### 3.8.2.1 Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['summary_polarity_score'] = summaries_train_df['text'].apply(lambda x: polarity(x))\n",
    "\n",
    "summaries_train_df['summary_polarity_score'].iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Student summaries polarity score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='polarity score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveness_student_summary = summaries_train_df['summary_polarity_score'].apply(lambda x: sentiment(x))\n",
    "\n",
    "positiveness_student_summary.iplot(\n",
    "    kind='hist',\n",
    "    bins=3,\n",
    "    layout=dict(\n",
    "        title='Students summary sentiment',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='sentiment'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_8_2_2\"></a>\n",
    "- ##### 3.8.2.2 Subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['summary_subjectivity_score'] = summaries_train_df['text'].apply(lambda x: subjectivity(x))\n",
    "\n",
    "summaries_train_df['summary_subjectivity_score'].iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Student summaries subjectivity score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='subjectivity score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to prompts, student summaries are substantially less neutral (i.e. more positive or negative) and are more opinionated. However, a large peak in subjectivity score exists at 0 which shows that some students managed to stay objective in their summaries. It could be beneficial to investigate whether there is a correlation between final grade (score) student was given and subjectivity/polarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_8_2_3\"></a>\n",
    "- ##### 3.8.1.3 Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['summary_readability_score'] = summaries_train_df['text'].apply(lambda x: flesch_reading_ease(x))\n",
    "\n",
    "summaries_train_df['summary_readability_score'].iplot(\n",
    "    kind='hist',\n",
    "    bins=120,\n",
    "    layout=dict(\n",
    "        title='Student summaries subjectivity score distribution',\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(title='readability score'),\n",
    "        yaxis=dict(title='count')\n",
    "    ),\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to prompt text, students' summaries are written in less advanced language and are much easier to read with mean readability score of 73. However, the distribution is negatively skewed with pronounced left tail. This could indicate some correlation between the language complexity students used and the grade they were awarded (possibly higher grade than average) for their summaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3_9\"></a>\n",
    "#### 3.9 Spelling Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of spelling mistakes could be correlated to the wording score awarded to each summary. It would, therefore, be beneficial to provide such analysis. This will be only done for student summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "sc = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_errors(word_list):\n",
    "\n",
    "    misspelled = sc.unknown(word_list)\n",
    "    percentage_of_misspelled = len(misspelled)/len(word_list)\n",
    "    \n",
    "    return percentage_of_misspelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['misspelled_words_percentage'] = summaries_train_df['text_tokenized'].apply(lambda x: spelling_errors(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CommonLit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
